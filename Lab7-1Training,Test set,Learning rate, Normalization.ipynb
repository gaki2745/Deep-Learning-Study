{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test datasets\n",
    "\n",
    "반드시 가지고 있는 data set 을 tarining 과 test로 나누어 학습을 한다.\n",
    "Training set을 가지고 Model을 학습하고 학습이 완료가 되었으면 Test set을 이용하여 모델을 테스트 한다.(단 Model은 Test set을 처음 접하는 데이터 셋이어야 한다)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기존의 cost 함수와 hypothesis optimizer는 동일 \n",
    "#Accuracy를 산출하는 과정 또한 동일하다 \n",
    "# 다른점은 train 시에는 반드시 Training set 만 사용하여 학습 하고 Test는 Test set 을 이용한다는 것이다.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer],\n",
    "                                     feed_dict = {X: x_data, Y: y_data})\n",
    "        \n",
    "        # predict\n",
    "        print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "        # Calculate the accuracy\n",
    "        print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n",
    "#학습한 모델을 테스트 할 때에는 test라고 분류한 데이터를 사용하여 테스트 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate: NaN!\n",
    "\n",
    "Learning rate 를 설정하는 것은 매우 중요하다.\n",
    "Learning rate 이란? Gradient Descent 알고리즘에서 cost function이 minimun이 되는 최적의 해를 찾아가는 과정에서 그래프의 경사면을 타고 반복적으로 내려올때 \n",
    "다음 point를 어느정도로 옮길지를 결정하는 것을 learning rate 라고 한다 즉 point 의 step 이다.\n",
    "\n",
    "<img width=\"954\" alt=\"2018-11-18 12 08 48\" src=\"https://user-images.githubusercontent.com/33486820/48662689-be305f00-eac8-11e8-9f99-70c85b70d54c.png\">\n",
    "<br>\n",
    "Large learning rate setting : Overshooting\n",
    "- learning set이 너무 크게 되면 step 이 너무 크게 되고 위의 좌측 그래프 처럼 step 이 커지게 된다 . 이는 결과 적으로 한쪽면에서 cost가 가장 작은 값으로 경사면을 타고 내려가야 하는데 반대편 경사면으로 올라가버리는 상황이발생하게 되고 이런 잘못된 학습을 반복적으로 하면 결국 경사면을 올라가버리는 현상까지 발생할수 있다. 즉 밥그릇 그래프를 탈출해버린다.\n",
    "Small learning rate setting\n",
    "- 이와 반대로 너무 작게 learning rate 를 설정하게 되면 경사면을 타고 내려가는 시간이 너무 오래 걸린다 . 일반적으로 조금씩 이동하는 것이 단점은 되지 않는데 머신러닝 처럼 몇만 몇십만등 큰 횟수의 반복을 해야하는 상황에는 학습에 치명적인 영향을 끼친다. 즉 학습시간이 기하 급수적으로 늘어난다는 것이다.\n",
    "\n",
    "결론: learning rate 는 설계하고자 하는 model 에 따라 data 에 따라 다르기 때문에 정해진 값은 없지만 대체적으로 0.01을 설정하여 적용하게 된다.\n",
    "최적의 learning rate를 선택하기 위해서는 overshooting 과 작은 값으로 했을때의 학습시간이 느려지는 것을 직접 몸으로 경험해보고 설정하는것이 적절하다.\n",
    "\n",
    "## Non-normalized input(NaN)\n",
    "\n",
    "학습 이전에 input data 에 대한 normalization 은 필수 적이다. 추후에 인공신경망 구축시에도 마찬가지\n",
    "\n",
    "### Min-Max  sale 을 사용하자!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
