{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 그래프를 빌드\n",
    "#2. sess.run(op, feed_dict={x: x_data}) 를 통해 그래프를 실행\n",
    "#3. 실행결과가 그래프를 업데이트 하거나 결과값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "#placeholder를 사용해서 그래프를 실행시킬때 값을 넘겨 줄수 있다\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')#텐서플로우가 자체적으로 변경하는 변수 즉 사용하는 변수또는 텐서플로우가 학습시킬수 있는 변수\n",
    "b = tf.Variable(tf.random_normal([1]), name ='bias')\n",
    "\n",
    "hypothesis = x_train * W + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))  #cost function 작성\n",
    "#reduce_mean( 평균 계산 함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression 의 핵심 -> cost 를 minimize 시키는 것\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost) # graph 상에서의 node가 된다. --> 를 실행 시켜야 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2342354 [0.09872963] [0.97024876]\n",
      "20 0.18033391 [0.48208013] [1.0746548]\n",
      "40 0.15525387 [0.5388896] [1.0384334]\n",
      "60 0.14092676 [0.56365174] [0.990991]\n",
      "80 0.12799127 [0.58445287] [0.9445477]\n",
      "100 0.11624372 [0.6040101] [0.90016973]\n",
      "120 0.10557439 [0.6226228] [0.8578662]\n",
      "140 0.09588439 [0.6403584] [0.8175497]\n",
      "160 0.08708372 [0.6572603] [0.7791279]\n",
      "180 0.07909083 [0.6733678] [0.7425118]\n",
      "200 0.07183156 [0.68871826] [0.7076165]\n",
      "220 0.065238565 [0.70334727] [0.6743612]\n",
      "240 0.05925077 [0.7172889] [0.6426687]\n",
      "260 0.053812455 [0.7305753] [0.6124657]\n",
      "280 0.048873346 [0.74323726] [0.5836821]\n",
      "300 0.04438759 [0.7553042] [0.55625117]\n",
      "320 0.040313493 [0.766804] [0.53010935]\n",
      "340 0.03661334 [0.7777633] [0.5051962]\n",
      "360 0.033252824 [0.7882076] [0.4814539]\n",
      "380 0.030200766 [0.7981611] [0.45882735]\n",
      "400 0.027428806 [0.8076468] [0.43726417]\n",
      "420 0.024911284 [0.8166867] [0.41671437]\n",
      "440 0.022624841 [0.8253017] [0.39713034]\n",
      "460 0.020548243 [0.8335119] [0.37846664]\n",
      "480 0.018662238 [0.84133625] [0.36068007]\n",
      "500 0.01694935 [0.8487928] [0.3437295]\n",
      "520 0.0153936595 [0.855899] [0.32757545]\n",
      "540 0.013980788 [0.86267126] [0.31218058]\n",
      "560 0.01269756 [0.8691251] [0.29750925]\n",
      "580 0.01153214 [0.8752758] [0.28352737]\n",
      "600 0.010473677 [0.8811373] [0.27020273]\n",
      "620 0.009512362 [0.88672346] [0.25750422]\n",
      "640 0.008639271 [0.89204705] [0.24540244]\n",
      "660 0.00784633 [0.89712036] [0.23386945]\n",
      "680 0.0071261493 [0.9019554] [0.22287843]\n",
      "700 0.006472089 [0.9065631] [0.21240401]\n",
      "720 0.005878059 [0.91095436] [0.2024218]\n",
      "740 0.0053385454 [0.91513914] [0.1929087]\n",
      "760 0.004848561 [0.9191272] [0.18384273]\n",
      "780 0.004403533 [0.922928] [0.1752028]\n",
      "800 0.003999365 [0.9265501] [0.16696894]\n",
      "820 0.0036322838 [0.930002] [0.159122]\n",
      "840 0.003298902 [0.9332916] [0.15164383]\n",
      "860 0.0029961143 [0.93642664] [0.14451718]\n",
      "880 0.0027211246 [0.9394142] [0.13772549]\n",
      "900 0.0024713676 [0.94226164] [0.13125291]\n",
      "920 0.0022445372 [0.94497514] [0.12508447]\n",
      "940 0.0020385238 [0.9475611] [0.11920596]\n",
      "960 0.0018514158 [0.9500256] [0.11360367]\n",
      "980 0.0016814846 [0.9523742] [0.10826468]\n",
      "1000 0.0015271549 [0.95461243] [0.10317662]\n",
      "1020 0.0013869838 [0.9567455] [0.09832768]\n",
      "1040 0.0012596828 [0.9587783] [0.09370663]\n",
      "1060 0.001144057 [0.9607156] [0.08930272]\n",
      "1080 0.0010390537 [0.9625618] [0.08510582]\n",
      "1100 0.00094368245 [0.9643212] [0.08110617]\n",
      "1120 0.000857069 [0.965998] [0.0772945]\n",
      "1140 0.0007784055 [0.967596] [0.07366193]\n",
      "1160 0.0007069611 [0.96911883] [0.07020008]\n",
      "1180 0.0006420706 [0.97057027] [0.06690089]\n",
      "1200 0.00058314047 [0.97195333] [0.06375676]\n",
      "1220 0.000529614 [0.9732714] [0.06076042]\n",
      "1240 0.00048100518 [0.97452754] [0.05790488]\n",
      "1260 0.0004368568 [0.97572464] [0.05518356]\n",
      "1280 0.00039675995 [0.97686553] [0.05259014]\n",
      "1300 0.00036034288 [0.9779527] [0.0501186]\n",
      "1320 0.0003272714 [0.9789888] [0.04776322]\n",
      "1340 0.00029723233 [0.9799763] [0.0455186]\n",
      "1360 0.00026995206 [0.98091733] [0.04337936]\n",
      "1380 0.00024517384 [0.9818142] [0.04134068]\n",
      "1400 0.00022267166 [0.9826689] [0.0393978]\n",
      "1420 0.00020223299 [0.9834834] [0.03754622]\n",
      "1440 0.00018367234 [0.98425955] [0.03578167]\n",
      "1460 0.00016681211 [0.9849993] [0.0341001]\n",
      "1480 0.00015150383 [0.9857043] [0.03249751]\n",
      "1500 0.00013759629 [0.9863761] [0.03097024]\n",
      "1520 0.00012496747 [0.9870164] [0.02951477]\n",
      "1540 0.00011349778 [0.98762655] [0.02812771]\n",
      "1560 0.00010307998 [0.98820806] [0.02680581]\n",
      "1580 9.361878e-05 [0.9887622] [0.02554607]\n",
      "1600 8.50269e-05 [0.9892903] [0.02434553]\n",
      "1620 7.7222256e-05 [0.9897938] [0.02320136]\n",
      "1640 7.013456e-05 [0.9902735] [0.02211089]\n",
      "1660 6.3697065e-05 [0.9907306] [0.02107171]\n",
      "1680 5.7850313e-05 [0.9911662] [0.0200814]\n",
      "1700 5.2541134e-05 [0.9915813] [0.01913763]\n",
      "1720 4.7717793e-05 [0.991977] [0.01823822]\n",
      "1740 4.333765e-05 [0.99235404] [0.01738109]\n",
      "1760 3.936092e-05 [0.9927134] [0.01656423]\n",
      "1780 3.5748337e-05 [0.9930558] [0.01578576]\n",
      "1800 3.2466847e-05 [0.99338216] [0.01504391]\n",
      "1820 2.9486968e-05 [0.9936932] [0.01433691]\n",
      "1840 2.678055e-05 [0.9939896] [0.01366313]\n",
      "1860 2.4322697e-05 [0.99427205] [0.01302102]\n",
      "1880 2.2090055e-05 [0.9945412] [0.01240907]\n",
      "1900 2.0062944e-05 [0.9947977] [0.01182589]\n",
      "1920 1.822147e-05 [0.9950422] [0.01127017]\n",
      "1940 1.6548756e-05 [0.9952752] [0.01074051]\n",
      "1960 1.5029615e-05 [0.9954973] [0.01023576]\n",
      "1980 1.3650802e-05 [0.9957089] [0.00975469]\n",
      "2000 1.23980535e-05 [0.9959105] [0.00929626]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습이 진행 될수 록  W 가 1에 가까워지고 cost값이 optrimize 되는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder 사용하는 이유 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-87fd64fcef06>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-87fd64fcef06>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    .\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[Node]) # 1차원 Array 에 사이즈는 정해지지 않았다 즉 아무값이나 가능\n",
    "Y = tf.placeholder(tf.float32, shape=[Node])\n",
    ".\n",
    ".\n",
    ".\n",
    "(동일)\n",
    "\n",
    "for step in range(2000):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost,W,b,train], feed_dict = {X: [1, 2, 3, 4, 5], Y: [2.1, 3.1, 4.1 ,5.1, 6.1]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val, W_val , b_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결론:  ( supervised learing인 Linear regression에 대해) 주어진 x ,y 의 training set이 주어지고  learing을 통해 hypothesis 식을 만족시키는 W와 b 를 구할수 있다.\n",
    "#학습 후  기존의 그래프를 업데이트 시키고  또한 원하는 값을 반환 할 수 있다.\n",
    "# print(sess.run(hypothesis, feed_dict = {X: [1.5, 3.5]}))  시 에는 2.6 과 4.6 에 근접하는 값을  학습을 통해 얻을 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
